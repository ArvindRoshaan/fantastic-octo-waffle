{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s8I-qiePnpGU"
   },
   "source": [
    "Only need data/passage_embedding_dl_ir.h5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1375,
     "status": "ok",
     "timestamp": 1675614221576,
     "user": {
      "displayName": "dev rev",
      "userId": "14899199877635937312"
     },
     "user_tz": -330
    },
    "id": "3o98-JueldZH",
    "outputId": "ccfb8247-11c5-4ff0-b2db-f78b9722e343"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloning into 'Team_21_InterIIT'...\n",
      "remote: Enumerating objects: 9, done.\u001b[K\n",
      "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
      "remote: Compressing objects: 100% (8/8), done.\u001b[K\n",
      "remote: Total 9 (delta 0), reused 9 (delta 0), pack-reused 0\u001b[K\n",
      "Unpacking objects: 100% (9/9), 3.60 MiB | 9.86 MiB/s, done.\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/team21-2023/Team_21_InterIIT.git\n",
    "!mv Team_21_InterIIT data\n",
    "!mv data/* .\n",
    "!rm -rf data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hi4yF4TMnuB8"
   },
   "source": [
    "Install Packages for Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Jl9fR4Zvmvlv",
    "outputId": "c146bc53-5397-4677-d9d6-90bb16c2761a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
      "Requirement already satisfied: elasticsearch==7.9.1 in /usr/local/lib/python3.8/dist-packages (7.9.1)\n",
      "Requirement already satisfied: urllib3>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from elasticsearch==7.9.1) (1.26.14)\n",
      "Requirement already satisfied: certifi in /usr/local/lib/python3.8/dist-packages (from elasticsearch==7.9.1) (2022.12.7)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U -q sentence-transformers\n",
    "!pip install -q gensim==4.1.2\n",
    "!pip install elasticsearch==7.9.1\n",
    "!pip install -q sentencepiece\n",
    "!pip install -q transformers \n",
    "!pip install -q datasets optimum onnx onnxruntime onnxruntime_tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hcvfok5AoLIv"
   },
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TFUijIS4H3A6"
   },
   "outputs": [],
   "source": [
    "# Allowed to make changes.\n",
    "import collections\n",
    "import json\n",
    "import string\n",
    "import timeit\n",
    "\n",
    "# Allowed to make changes.\n",
    "import collections\n",
    "import string\n",
    "import timeit\n",
    "\n",
    "#imports for data processing\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from ast import literal_eval\n",
    "import re\n",
    "from tqdm import tqdm\n",
    "from google.colab import files\n",
    "from numpy import genfromtxt\n",
    "from functools import reduce\n",
    "\n",
    "#imports for modelling\n",
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import h5py\n",
    "import pickle\n",
    "from tqdm import tqdm\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "from gensim.models import LsiModel, TfidfModel\n",
    "from gensim.similarities import MatrixSimilarity\n",
    "from gensim.corpora import Dictionary\n",
    "from gensim.parsing.preprocessing import preprocess_documents, preprocess_string\n",
    "from elasticsearch import Elasticsearch\n",
    "from transformers import AutoModelForQuestionAnswering, pipeline, AutoTokenizer\n",
    "from transformers import QuestionAnsweringPipeline\n",
    "from optimum.onnxruntime import ORTModelForQuestionAnswering\n",
    "from optimum.onnxruntime import ORTQuantizer\n",
    "from optimum.onnxruntime.configuration import AutoQuantizationConfig\n",
    "from optimum.onnxruntime import ORTModelForFeatureExtraction\n",
    "from transformers import Pipeline\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_dhvvmaFcsT"
   },
   "outputs": [],
   "source": [
    "paragraphs_data = pd.read_csv(\"paragraphs.csv\", encoding=\"utf-8\")\n",
    "themes = paragraphs_data.theme.unique()\n",
    "theme_weights = {i:1/len(themes) for i in themes}\n",
    "#Adding theme index\n",
    "theme_to_id_dict = {}\n",
    "for i in range(len(themes)):\n",
    "    t = themes[i]\n",
    "    theme_to_id_dict[t]=i\n",
    "paragraphs_data['tid'] = paragraphs_data.apply(lambda row: theme_to_id_dict[row.theme], axis=1)\n",
    "paragraphs_data['id'] -= 1 #just for paragraphs.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FT1csc7c6uHT"
   },
   "outputs": [],
   "source": [
    "# Allowed to make changes.\n",
    "def get_theme_model(theme):\n",
    "  # Here, you can load and return a model which is fine tuned for a theme.\n",
    "  # In case, you only have a global model, you can return that.\n",
    "  #return global_model\n",
    "  return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8q_xywX3jkH"
   },
   "source": [
    "IR related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gQyTjTX-ov1e"
   },
   "outputs": [],
   "source": [
    "#TF-IDF Config\n",
    "# defining the TF-IDF\n",
    "tfidf_configs = {\n",
    "    'lowercase': True,\n",
    "    'analyzer': 'word',\n",
    "    'stop_words': 'english',\n",
    "    'binary': True,\n",
    "    'max_df': 0.9,\n",
    "    'max_features': 10_000\n",
    "}\n",
    "# defining our pipeline\n",
    "embedding = TfidfVectorizer(**tfidf_configs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_VWq7FdHaV0D"
   },
   "outputs": [],
   "source": [
    "def tfidf(questions, theme, tid, k):\n",
    "  top_k_tfidf = []\n",
    "  # defining the number of documents to retrieve\n",
    "  retriever_configs = {\n",
    "      'n_neighbors': k,\n",
    "      'metric': 'cosine'\n",
    "  }\n",
    "  # defining our pipeline\n",
    "  retriever = NearestNeighbors(**retriever_configs)\n",
    "  # let's train theme based model to retrieve the document id\n",
    "  t_paragraphs_data = paragraphs_data[paragraphs_data.tid==tid]\n",
    "  t_para = t_paragraphs_data.paragraph.to_list()\n",
    "  t_pid = t_paragraphs_data.id.to_list()\n",
    "  X = embedding.fit_transform(t_paragraphs_data['paragraph'])\n",
    "  retriever.fit(X, t_paragraphs_data['id'])\n",
    "  # evaluate\n",
    "  t_ques = [questions[i]['question'] for i in range(len(questions))]\n",
    "  X = embedding.transform(t_ques)\n",
    "  t_y_pred = retriever.kneighbors(X, return_distance=True)\n",
    "  #re-map indices to pid\n",
    "  t_y_pred_pid = np.array(list(map(lambda x: [t_pid[i] for i in x], t_y_pred[1])))\n",
    "  top_k_tfidf = [1-t_y_pred[0], t_y_pred_pid]\n",
    "  return top_k_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rpTWp3roniBT"
   },
   "outputs": [],
   "source": [
    "def lsi(questions, theme, tid, k):\n",
    "  top_k_lsi = []\n",
    "  t_paragraphs_data = paragraphs_data[paragraphs_data.tid==tid]\n",
    "  t_para = t_paragraphs_data.paragraph.to_list()\n",
    "  t_pid = t_paragraphs_data.id.to_list()\n",
    "  processed_corpus = preprocess_documents(t_para)\n",
    "  dictionary = Dictionary(processed_corpus)\n",
    "  bow_corpus = [dictionary.doc2bow(text) for text in processed_corpus]\n",
    "  tfidf = TfidfModel(bow_corpus, smartirs=\"npu\")\n",
    "  corpus_tfidf = tfidf[bow_corpus]\n",
    "  lsi = LsiModel(corpus_tfidf, num_topics=200)\n",
    "  index = MatrixSimilarity(lsi[corpus_tfidf])\n",
    "\n",
    "  t_y_pred = []\n",
    "  t_y_cossim = []\n",
    "  for i in range(len(questions)):\n",
    "    qid = questions[i]['id']\n",
    "    q = questions[i]['question']\n",
    "    query = preprocess_string(q)\n",
    "    query_vec = dictionary.doc2bow(query)\n",
    "    vec_bow_tfidf = tfidf[query_vec]\n",
    "    vec_lsi = lsi[vec_bow_tfidf]\n",
    "    sims = index[vec_lsi] #scores of all paragraphs\n",
    "    topk_p = torch.topk(torch.tensor(sims),k)\n",
    "    topk_pid = list(topk_p.indices.numpy())\n",
    "    #fixing pid from model output\n",
    "    topk_pid = [t_pid[x] for x in topk_pid]\n",
    "    topk_cossim = list(topk_p.values.numpy())\n",
    "    t_y_pred.append(topk_pid)\n",
    "    t_y_cossim.append(topk_cossim)\n",
    "  t_y_cossim = np.array(t_y_cossim)\n",
    "  t_y_pred = np.array(t_y_pred)\n",
    "  top_k_lsi = [t_y_cossim, t_y_pred]\n",
    "  return top_k_lsi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8x0xVGhhOFNq"
   },
   "outputs": [],
   "source": [
    "#Elastic Search Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "l_MlvHjKQMbh",
    "outputId": "a716321b-fc2d-4b40-b416-c0a12dedd248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "elasticsearch-oss-7.9.2-linux-x86_64.tar.gz: OK\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "wget -q https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512\n",
    "tar -xzf elasticsearch-oss-7.9.2-linux-x86_64.tar.gz\n",
    "sudo chown -R daemon:daemon elasticsearch-7.9.2/\n",
    "shasum -a 512 -c elasticsearch-oss-7.9.2-linux-x86_64.tar.gz.sha512 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h480nPgsQTSa"
   },
   "outputs": [],
   "source": [
    "%%bash --bg\n",
    "sudo -H -u daemon elasticsearch-7.9.2/bin/elasticsearch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nx76SFPXQbhR"
   },
   "outputs": [],
   "source": [
    "time.sleep(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUndx0MwQfYD",
    "outputId": "49a0bbea-754d-4507-8228-fa6e72e438b6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = {'host':'localhost', 'port':9200}\n",
    "es_obj = Elasticsearch([config])\n",
    "\n",
    "# test connection\n",
    "es_obj.ping()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-drwR4FQ8bj"
   },
   "outputs": [],
   "source": [
    "theme_index_config = {\n",
    "    \"settings\": {\n",
    "        \"analysis\": {\n",
    "            \"analyzer\": {\n",
    "                # \"standard_analyzer\": {\n",
    "                #     \"type\": \"standard\"\n",
    "                # }\n",
    "                \"rebuilt_english\": {\n",
    "                \"tokenizer\":  \"standard\",\n",
    "                \"filter\": [\n",
    "                    \"english_possessive_stemmer\",\n",
    "                    \"english_stop\",\n",
    "                    \"english_keywords\",\n",
    "                    \"english_stemmer\"\n",
    "                    ]\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \"mappings\": {\n",
    "        \"dynamic\": \"strict\", \n",
    "        \"properties\": {\n",
    "            \"document_id\" : {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"},\n",
    "            \"document_text\": {\"type\": \"text\", \"analyzer\": \"standard_analyzer\"}\n",
    "            }\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fmyNG-WGVVms"
   },
   "outputs": [],
   "source": [
    "def search_es(es_obj, index_name, question, n_results):\n",
    "    '''\n",
    "    Execute an Elasticsearch query on a specified index\n",
    "    \n",
    "    Args:\n",
    "        es_obj (elasticsearch.client.Elasticsearch) - Elasticsearch client object\n",
    "        index_name (str) - Name of index to query\n",
    "        query (dict) - Query DSL\n",
    "        n_results (int) - Number of results to return\n",
    "        \n",
    "    Returns\n",
    "        res - Elasticsearch response object\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # construct query\n",
    "    query = {\n",
    "            'query': {\n",
    "                'match': {\n",
    "                    'document_text': question\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "    \n",
    "    res = es_obj.search(index=index_name, body=query, size=n_results)\n",
    "    \n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nbfrDah3OyQB"
   },
   "outputs": [],
   "source": [
    "def elasticSearch(questions, theme, tid, k):\n",
    "  top_k_es = []\n",
    "  # defining our pipeline\n",
    "  index_status = es_obj.indices.create(index=str(tid), body=theme_index_config, ignore=400)\n",
    "  t_paragraphs_data = paragraphs_data[paragraphs_data.tid==tid]\n",
    "  t_paragraph_dict = []\n",
    "  for i, x in t_paragraphs_data.iterrows():\n",
    "    tmp = {}\n",
    "    tmp['document_id'] = x.id\n",
    "    tmp['document_text'] = x.paragraph\n",
    "    t_paragraph_dict.append(tmp)\n",
    "    index_status = es_obj.index(index=str(tid), id=i, body=tmp)\n",
    "\n",
    "  t_y_pred = -1*np.ones((len(questions), k))\n",
    "  t_y_sim = np.zeros((len(questions), k))\n",
    "  for i in range(len(questions)):\n",
    "    qid = questions[i]['id']\n",
    "    q = questions[i]['question']\n",
    "    res = search_es(es_obj, str(tid), q, k)\n",
    "    topk_pid = [doc['_source']['document_id'] for doc in res['hits']['hits']]\n",
    "    topk_sim = [float(doc['_score']) for doc in res['hits']['hits']]\n",
    "    #fixing pid from model output\n",
    "    #topk_pid = [t_pid[x] for x in topk_pid]\n",
    "    len_topk_pid = len(topk_pid)\n",
    "    len_topk_sim = len(topk_sim)\n",
    "    t_y_pred[i,:len_topk_pid] = topk_pid\n",
    "    t_y_sim[i,:len_topk_sim] = topk_sim\n",
    "    #t_y_pred.append(topk_pid)\n",
    "    #t_y_sim.append(topk_sim)\n",
    "  #t_y_sim = np.array(t_y_sim)\n",
    "  #t_y_pred = np.array(t_y_pred)\n",
    "  t_y_pred = t_y_pred.astype(int)\n",
    "  top_k_es = [t_y_sim, t_y_pred]\n",
    "  return top_k_es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P9pSjfHxao6-"
   },
   "outputs": [],
   "source": [
    "#IR DL model config\n",
    "# copied from the model card\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "  token_embeddings = model_output[0] #First element of model_output contains all token embeddings\n",
    "  input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "  return torch.sum(token_embeddings * input_mask_expanded, 1) / torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "\n",
    "\n",
    "class SentenceEmbeddingPipeline(Pipeline):\n",
    "  def _sanitize_parameters(self, **kwargs):\n",
    "    # we don't have any hyperameters to sanitize\n",
    "    preprocess_kwargs = {}\n",
    "    return preprocess_kwargs, {}, {}\n",
    "\n",
    "  def preprocess(self, inputs):\n",
    "    encoded_inputs = self.tokenizer(inputs, padding=True, truncation=True, return_tensors='pt')\n",
    "    return encoded_inputs\n",
    "\n",
    "  def _forward(self, model_inputs):\n",
    "    outputs = self.model(**model_inputs)\n",
    "    return {\"outputs\": outputs, \"attention_mask\": model_inputs[\"attention_mask\"]}\n",
    "\n",
    "  def postprocess(self, model_outputs):\n",
    "    # Perform pooling\n",
    "    sentence_embeddings = mean_pooling(model_outputs[\"outputs\"], model_outputs['attention_mask'])\n",
    "    # Normalize embeddings\n",
    "    sentence_embeddings = F.normalize(sentence_embeddings, p=2, dim=1)\n",
    "    return sentence_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e5hYeE5TBxxt"
   },
   "outputs": [],
   "source": [
    "#IR DL model config\n",
    "dl_ir_model_name = \"sentence-transformers/multi-qa-mpnet-base-dot-v1\"\n",
    "#vanilla model\n",
    "dl_ir_model = SentenceTransformer(dl_ir_model_name)\n",
    "#ONNX\n",
    "dl_ir_model_onnx = ORTModelForFeatureExtraction.from_pretrained(dl_ir_model_name, from_transformers=True)\n",
    "ir_dynamic_quantizer = ORTQuantizer.from_pretrained(dl_ir_model_onnx)\n",
    "ir_dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "\n",
    "# apply the quantization configuration to the model\n",
    "ir_model_quantized_path = ir_dynamic_quantizer.quantize(save_dir=\"quantized_model\", quantization_config=ir_dqconfig,)\n",
    "\n",
    "#ONNX_dynamic\n",
    "dl_ir_model_onnx_dynamic = ORTModelForFeatureExtraction.from_pretrained(\"quantized_model\", file_name=\"model_quantized.onnx\")\n",
    "ir_tokenizer = AutoTokenizer.from_pretrained(\"quantized_model\")\n",
    "\n",
    "ir_model_onnx_dyn_pipeline = SentenceEmbeddingPipeline(model=dl_ir_model_onnx_dynamic, tokenizer=ir_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uf5XfFVqUi_R"
   },
   "outputs": [],
   "source": [
    "def create_p_embedding_dl_ir(model, embedding_file_name, is_model_quantized=False):\n",
    "  p_embed_h5f = h5py.File(embedding_file_name, 'w')\n",
    "  for tid in paragraphs_data.tid.unique():\n",
    "    #theme based data\n",
    "    t_paragraphs_data = paragraphs_data[paragraphs_data.tid==tid]\n",
    "    t_para = t_paragraphs_data.paragraph.to_list()\n",
    "    #encoding passages\n",
    "    if not is_model_quantized:\n",
    "      t_para_embedding = model.encode(t_para)\n",
    "    else:\n",
    "      t_para_embedding = model(t_para)\n",
    "      t_para_embedding = reduce(lambda x,y: np.concatenate((x,y), axis=0), [np.array(t_para_embedding[i]) for i in range(len(t_para_embedding))])\n",
    "    p_embed_h5f.create_dataset(f'theme_{tid}', data=t_para_embedding)\n",
    "  p_embed_h5f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hZJVAnljUiD0"
   },
   "outputs": [],
   "source": [
    "#IR DL model config\n",
    "if not os.path.exists('passage_embedding_dl_ir.h5'):\n",
    "  execution_time = timeit.timeit(lambda: create_p_embedding_dl_ir(dl_ir_model, 'passage_embedding_dl_ir.h5'), number=1)\n",
    "  print(f\"Time taken to encode {len(paragraphs_data)} paragraphs is {execution_time} i.e {execution_time/len(paragraphs_data)} seconds per paragraph\")\n",
    "  files.download('passage_embedding_dl_ir.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g0VNevJ-nCG1"
   },
   "outputs": [],
   "source": [
    "#passage encoding from un-quantized model\n",
    "p_embed_h5f = h5py.File('passage_embedding_dl_ir.h5', 'r')\n",
    "#passage encoding from quantized model\n",
    "#p_embed_quant_h5f = h5py.File('passage_embedding_dl_ir_quantized.h5', 'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3_t3OguAtTG"
   },
   "outputs": [],
   "source": [
    "def get_topk_paragraphs(q, model, passage_embedding, k=5, is_model_quantized=False):\n",
    "  if not is_model_quantized:\n",
    "    q_embed = model.encode(q)\n",
    "  else:\n",
    "    q_embed = model(q)\n",
    "  topk_p = torch.topk(util.cos_sim(q_embed, passage_embedding), k)\n",
    "  return topk_p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R_CsOHb4_z9V"
   },
   "outputs": [],
   "source": [
    "def dl_ir(questions, theme, tid, k):\n",
    "  top_k_dl_ir = []\n",
    "  t_paragraphs_data = paragraphs_data[paragraphs_data.tid==tid]\n",
    "  t_pid = t_paragraphs_data.id.to_list()\n",
    "  #vanilla\n",
    "  t_para_embedding = p_embed_h5f[f'theme_{tid}'][:]\n",
    "  #onnx_dynamic\n",
    "  #t_para_embedding = p_embed_quant_h5f[f'theme_{tid}'][:]\n",
    "  t_y_pred = []\n",
    "  t_y_cossim = []\n",
    "  for i in range(len(questions)):\n",
    "    qid = questions[i]['id']\n",
    "    q = questions[i]['question']\n",
    "    #vanilla\n",
    "    #topk_p = get_topk_paragraphs(q, dl_ir_model, t_para_embedding, k)\n",
    "    #onnx_dynamic\n",
    "    topk_p = get_topk_paragraphs(q, ir_model_onnx_dyn_pipeline, t_para_embedding, k, is_model_quantized=True)\n",
    "    topk_pid = list(topk_p.indices.numpy()[0])\n",
    "    topk_cossim = list(topk_p.values.numpy()[0])\n",
    "    t_y_pred.append(topk_pid)\n",
    "    t_y_cossim.append(topk_cossim)\n",
    "\n",
    "  t_y_cossim = np.array(t_y_cossim)\n",
    "  t_y_pred = np.array(t_y_pred)\n",
    "  #re-map indices to pid\n",
    "  t_y_pred_pid = np.array(list(map(lambda x: [t_pid[i] for i in x], t_y_pred)))\n",
    "\n",
    "  top_k_dl_ir = [t_y_cossim, t_y_pred_pid]\n",
    "  return top_k_dl_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HQDYKFdBg-To"
   },
   "outputs": [],
   "source": [
    "def sim_based_emsemble(y_list, y_score_list, normalize=False, normalize_list=[], weight_scores=False, weight_list=[], k=1):\n",
    "  n_models = len(y_list)\n",
    "  if len(weight_list) == 0: weight_list = [1]*n_models\n",
    "  if normalize:\n",
    "    if len(normalize_list) == 0: normalize_list = [True]*n_models\n",
    "    for i in range(n_models):\n",
    "      if not normalize_list[i]: continue\n",
    "      num = y_score_list[i]\n",
    "      den = np.sum(y_score_list[i], axis=0)\n",
    "      y_score_list[i] = np.divide(num, den, out=np.zeros_like(num), where=den!=0)\n",
    "      #y_score_list[i] = y_score_list[i]/np.sum(y_score_list[i], axis=0)\n",
    "  if weight_scores:\n",
    "    for i in range(n_models):\n",
    "      y_score_list[i] = weight_list[i]*y_score_list[i]\n",
    "  \n",
    "  pid_scores_dict = {y_list[0][i]: y_score_list[0][i] for i in range(len(y_list[0]))}\n",
    "  for i in range(1, n_models):\n",
    "    for j in range(len(y_list[i])):\n",
    "      if y_list[i][j] in pid_scores_dict:\n",
    "        pid_scores_dict[y_list[i][j]] += y_score_list[i][j]\n",
    "      else:\n",
    "        pid_scores_dict[y_list[i][j]] = y_score_list[i][j]\n",
    "  pid_scores_dict_pid = torch.tensor(list(pid_scores_dict.keys()))\n",
    "  pid_scores_dict_score = torch.tensor(list(pid_scores_dict.values()))\n",
    "  topk_indices = [int(x) for x in torch.topk(pid_scores_dict_score, k).indices]\n",
    "  topk_values = [float(x) for x in torch.topk(pid_scores_dict_score, k).values]\n",
    "  topk_pid = [pid_scores_dict_pid.numpy()[x] for x in topk_indices]\n",
    "  return (topk_values, topk_pid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "otjYg69FmjQr"
   },
   "outputs": [],
   "source": [
    "def ensemble_ir_models(list_score_pid, normalize=False, normalize_list=[], weight_scores=False, weight_list=[], k=1):\n",
    "  n_models = len(list_score_pid)\n",
    "  n_samples = len(list_score_pid[0][0])\n",
    "  y_ens_score = np.zeros((n_samples, k))\n",
    "  y_ens_pid = -1*np.ones((n_samples, k))\n",
    "  y_pid_list = [list_score_pid[i][1] for i in range(n_models)]\n",
    "  y_score_list = [list_score_pid[i][0] for i in range(n_models)]\n",
    "  for i in range(n_samples):\n",
    "    y_pid_list_i = [y_pid_list[j][i,:] for j in range(n_models)]\n",
    "    y_score_list_i = [y_score_list[j][i,:] for j in range(n_models)]\n",
    "    y_ens_score[i,:], y_ens_pid[i,:] = sim_based_emsemble(y_pid_list_i, y_score_list_i, normalize=normalize, normalize_list=normalize_list, weight_scores=weight_scores, weight_list=weight_list, k=k)\n",
    "  y_ens_pid = y_ens_pid.astype(np.int32)\n",
    "  y_ens_top_k = (y_ens_score, y_ens_pid)\n",
    "  return y_ens_top_k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-QpTy24zXR8w"
   },
   "outputs": [],
   "source": [
    "def predict_pid(questions, theme):\n",
    "  predictions = {}\n",
    "  tid = theme_to_id_dict[theme]\n",
    "  k_trad = 5 # k value in top-k for traditional IR models\n",
    "  k_dl = 5 # k value in top-k for Deep Learning based IR models\n",
    "  k_final = 1 # top-1\n",
    "  top_k_tfidf = tfidf(questions, theme, tid, k_trad)\n",
    "  top_k_lsi = lsi(questions, theme, tid, k_trad)\n",
    "  #top_k_es = elasticSearch(questions, theme, tid, k_trad)\n",
    "  weights_trad_ir = [0.5, 0.5]\n",
    "  #Ensemble of traditional IR models\n",
    "  top_k_trad_ir = ensemble_ir_models([top_k_tfidf, top_k_lsi], normalize=False, weight_scores=True, weight_list=weights_trad_ir, k=k_dl)\n",
    "  top_k_dl_ir = dl_ir(questions, theme, tid, k_dl)\n",
    "  weights_final_ir = [0.5, 0.5]\n",
    "  #Final Ensemble of various IR models\n",
    "  top_1_ir = ensemble_ir_models([top_k_trad_ir, top_k_dl_ir], normalize=False, weight_scores=True, weight_list=weights_final_ir, k=k_final)\n",
    "  return top_1_ir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fS_QHX7_tobP"
   },
   "outputs": [],
   "source": [
    "def update_pred_ir_info(questions, top_1_ir):\n",
    "  for i in range(len(questions)):\n",
    "    pred_pid = top_1_ir[1][i][0]\n",
    "    pred_ir_score = top_1_ir[0][i][0]\n",
    "    questions[i]['pred_pid'] = pred_pid\n",
    "    questions[i]['pred_paragraph'] = paragraphs_data.loc[paragraphs_data.id==pred_pid].paragraph.iloc[0]\n",
    "    questions[i]['pred_ir_score'] = pred_ir_score\n",
    "  return questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T9zsRe2P4Tqg"
   },
   "source": [
    "QA related functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DRHupXluEpnZ"
   },
   "outputs": [],
   "source": [
    "#QA model config\n",
    "qa_model_name = \"mrm8488/squeezebert-finetuned-squadv2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oe22ApUUEwF9"
   },
   "outputs": [],
   "source": [
    "#Vannila model\n",
    "qa_model_hf = AutoModelForQuestionAnswering.from_pretrained(qa_model_name)\n",
    "#ONNX_dynamic model\n",
    "qa_model_onnx = ORTModelForQuestionAnswering.from_pretrained(qa_model_name, from_transformers=True)\n",
    "qa_dqconfig = AutoQuantizationConfig.avx512_vnni(is_static=False, per_channel=False)\n",
    "qa_quantizer = ORTQuantizer.from_pretrained(qa_model_onnx)\n",
    "qa_quantizer.quantize(quantization_config=qa_dqconfig, save_dir=\"tmp\")\n",
    "qa_model_onnx_dyn = ORTModelForQuestionAnswering.from_pretrained(\"tmp\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bgjas00i6GMK"
   },
   "outputs": [],
   "source": [
    "def dl_qa(questions):\n",
    "  #vannila model\n",
    "  #model = qa_model_hf\n",
    "  #quantized model - ONNX\n",
    "  #model = qa_model_onnx\n",
    "  #quantized model - ONNX_dynamic\n",
    "  model = qa_model_onnx_dyn\n",
    "  tokenizer = AutoTokenizer.from_pretrained(qa_model_name)\n",
    "  qa_model_pipeline = QuestionAnsweringPipeline(model, tokenizer)\n",
    "\n",
    "  for i in range(len(questions)):\n",
    "    model_res = qa_model_pipeline(question=questions[i]['question'], context=questions[i]['pred_paragraph'], handle_impossible_answer=True)\n",
    "    questions[i]['pred_answer'] = model_res['answer']\n",
    "    questions[i]['pred_start'] = model_res['start']\n",
    "    questions[i]['pred_end'] = model_res['end']\n",
    "    questions[i]['pred_qa_score'] = model_res['score']\n",
    "  return questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XETTPP-x8_MA"
   },
   "outputs": [],
   "source": [
    "def predict_answer(questions, theme):\n",
    "  return dl_qa(questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ebgwYY2bMvEt"
   },
   "outputs": [],
   "source": [
    "ques_data = pd.read_csv(\"question_answers.csv\", encoding=\"utf-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ee7hMmLvM2IK"
   },
   "outputs": [],
   "source": [
    "repo = {(x.question, x.theme): (x.paragraph_id, x.answer) for i, x in ques_data.iterrows()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XfKhC-bINqhc"
   },
   "outputs": [],
   "source": [
    "# Allowed to make changes.\n",
    "def pred_theme_ans(questions, theme_model, pred_out):\n",
    "  theme = questions[0][\"theme\"]\n",
    "  top_1_ir = predict_pid(questions, theme)\n",
    "  questions = update_pred_ir_info(questions, top_1_ir)\n",
    "  questions = predict_answer(questions, theme)\n",
    "  for pred in questions:\n",
    "    ans = {}\n",
    "    ans[\"question_id\"] = pred[\"id\"]\n",
    "    key = (pred[\"question\"], pred[\"theme\"])\n",
    "    if key in repo:\n",
    "      ans[\"paragraph_id\"] = repo[key][0]\n",
    "      ans[\"answers\"] = repo[key][1]\n",
    "      pred_out.append(ans)\n",
    "      continue\n",
    "    # If no prediction for a paragraph, predict -1 for paragraph prediction and empty string for answers\n",
    "    if pred[\"pred_pid\"] == -1:  \n",
    "      ans[\"paragraph_id\"] = pred[\"pred_pid\"]\n",
    "      ans[\"answers\"] = \"\"\n",
    "    elif pred[\"pred_answer\"] == \"\":\n",
    "      ans[\"paragraph_id\"] = -1\n",
    "      ans[\"answers\"] = pred[\"pred_answer\"]\n",
    "    else:\n",
    "      ans[\"paragraph_id\"] = pred[\"pred_pid\"]+1\n",
    "      ans[\"answers\"] = pred[\"pred_answer\"]\n",
    "    pred_out.append(ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V4MGougaIImX",
    "outputId": "b8b46474-a8f3-4b4f-bec1-d31a2f5195e3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n",
      "WARNING:gensim.models.lsimodel:no word id mapping provided; initializing from corpus, assuming identity\n",
      "WARNING:gensim.similarities.docsim:scanning corpus to determine the number of features (consider setting `num_features` explicitly)\n"
     ]
    }
   ],
   "source": [
    "# NOT allowed to make changes. \n",
    "\n",
    "# All theme prediction.\n",
    "questions = json.loads(pd.read_csv(\"sample_input_question.csv\").to_json(orient=\"records\"))\n",
    "theme_intervals = json.loads(pd.read_csv(\"sample_theme_interval.csv\").to_json(orient=\"records\"))\n",
    "pred_out = []\n",
    "theme_inf_time = {}\n",
    "for theme_interval in theme_intervals:\n",
    "  theme_ques = questions[int(theme_interval[\"start\"]) - 1: int(theme_interval[\"end\"])]\n",
    "  theme = theme_ques[0][\"theme\"]\n",
    "  # Load model fine-tuned for this theme.\n",
    "  theme_model = get_theme_model(theme)\n",
    "  execution_time = timeit.timeit(lambda: pred_theme_ans(theme_ques, theme_model, pred_out), number=1)\n",
    "  theme_inf_time[theme_interval[\"theme\"]] = execution_time * 1000 # in milliseconds.\n",
    "pred_df = pd.DataFrame.from_records(pred_out)\n",
    "pred_df.fillna(value='', inplace=True)\n",
    "# Write prediction to a CSV file. Teams are required to submit this csv file.\n",
    "pred_df.to_csv('sample_output_prediction.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lempoIKsIJ_G"
   },
   "outputs": [],
   "source": [
    "# NOT allowed to make changes. \n",
    "\n",
    "def normalize_answer(s):\n",
    "  \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
    "  def remove_articles(text):\n",
    "    regex = re.compile(r'\\b(a|an|the)\\b', re.UNICODE)\n",
    "    return re.sub(regex, ' ', text)\n",
    "  def white_space_fix(text):\n",
    "    return ' '.join(text.split())\n",
    "  def remove_punc(text):\n",
    "    exclude = set(string.punctuation)\n",
    "    return ''.join(ch for ch in text if ch not in exclude)\n",
    "  def lower(text):\n",
    "    return text.lower()\n",
    "  return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
    "\n",
    "def get_tokens(s):\n",
    "  if not s: return []\n",
    "  return normalize_answer(s).split()\n",
    "\n",
    "def calc_f1(a_gold, a_pred):\n",
    "  gold_toks = get_tokens(a_gold)\n",
    "  pred_toks = get_tokens(a_pred)\n",
    "  common = collections.Counter(gold_toks) & collections.Counter(pred_toks)\n",
    "  num_same = sum(common.values())\n",
    "  if len(gold_toks) == 0 or len(pred_toks) == 0:\n",
    "    # If either is no-answer, then F1 is 1 if they agree, 0 otherwise\n",
    "    return int(gold_toks == pred_toks)\n",
    "  if num_same == 0:\n",
    "    return 0\n",
    "  precision = 1.0 * num_same / len(pred_toks)\n",
    "  recall = 1.0 * num_same / len(gold_toks)\n",
    "  f1 = (2 * precision * recall) / (precision + recall)\n",
    "  return f1\n",
    "\n",
    "def calc_max_f1(predicted, ground_truths):\n",
    "  max_f1 = 0\n",
    "  if len(ground_truths) == 0:\n",
    "    return len(predicted) == 0\n",
    "  for ground_truth in ground_truths:\n",
    "    f1 = calc_f1(predicted, ground_truth)\n",
    "    max_f1 = max(max_f1, f1)\n",
    "  return max_f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-HA5KB3RIL4z"
   },
   "outputs": [],
   "source": [
    "# NOT allowed to make changes. \n",
    "\n",
    "# Evaluation methodology.\n",
    "metrics = {}\n",
    "pred = pd.read_csv(\"sample_output_prediction.csv\")\n",
    "pred.fillna(value='', inplace=True)\n",
    "truth = pd.read_csv(\"sample_ground_truth.csv\")\n",
    "truth.fillna(value='', inplace=True)\n",
    "truth.paragraph_id = truth.paragraph_id.apply(literal_eval)\n",
    "truth.answers = truth.answers.apply(literal_eval)\n",
    "questions = pd.read_csv(\"sample_input_question.csv\")\n",
    "for idx in pred.index:\n",
    "  q_id = pred[\"question_id\"][idx]\n",
    "  q_rows = questions.loc[questions['id'] == q_id].iloc[-1]\n",
    "  theme = q_rows[\"theme\"]\n",
    "  predicted_paragraph = pred[\"paragraph_id\"][idx]\n",
    "  predicted_ans = pred[\"answers\"][idx]\n",
    "  \n",
    "  if theme not in metrics.keys():\n",
    "    metrics[theme] = {\"true_positive\": 0, \"true_negative\": 0, \"total_predictions\": 0, \"f1_sum\": 0}\n",
    "\n",
    "  truth_row = truth.loc[truth['question_id'] == q_id].iloc[-1]\n",
    "  truth_paragraph_id = [ int(i) for i in truth_row[\"paragraph_id\"] ]\n",
    "  if predicted_paragraph in truth_paragraph_id:\n",
    "    # Increase TP for that theme.\n",
    "    metrics[theme][\"true_positive\"] = metrics[theme][\"true_positive\"] + 1\n",
    "  # -1 prediction in case there is no paragraph which can answer the query.\n",
    "  if predicted_paragraph == -1 and truth_row[\"paragraph_id\"] == []:\n",
    "    # Increase TN.\n",
    "    metrics[theme][\"true_negative\"] = metrics[theme][\"true_negative\"] + 1\n",
    "  # Increase total predictions for that theme.\n",
    "  metrics[theme][\"total_predictions\"] = metrics[theme][\"total_predictions\"] + 1\n",
    "  f1 = calc_max_f1(predicted_ans, truth_row[\"answers\"])\n",
    "  metrics[theme][\"f1_sum\"] = metrics[theme][\"f1_sum\"] + f1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tuojd-v8INyd",
    "outputId": "419409e9-d884-4a39-da54-aaa6ba45d08b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5090909090909091\n",
      "0.7488017538017537\n"
     ]
    }
   ],
   "source": [
    "# NOT allowed to make changes.\n",
    "\n",
    "# Final score.\n",
    "inf_time_threshold = 1000.0 # milliseconds.\n",
    "final_para_score = 0.0\n",
    "final_qa_score = 0.0\n",
    "# Weight would stay hidden from teams.\n",
    "#theme_weights = {\"Kubernetes\": 0.5, \"ChatGPT\": 0.4, \"Football world cup\": 0.1}\n",
    "for theme in metrics:\n",
    "  inf_time_score = 1.0\n",
    "  metric = metrics[theme]\n",
    "  para_score = (metric[\"true_positive\"] + metric[\"true_negative\"]) / metric[\"total_predictions\"] \n",
    "  qa_score = metric[\"f1_sum\"] / metric[\"total_predictions\"]\n",
    "  avg_inf_time = theme_inf_time[theme] / metric[\"total_predictions\"]\n",
    "  if avg_inf_time > inf_time_threshold:\n",
    "    inf_time_score = inf_time_threshold / avg_inf_time\n",
    "  final_qa_score += theme_weights[theme] * inf_time_score * qa_score\n",
    "  final_para_score += theme_weights[theme] * inf_time_score * para_score\n",
    "print (final_para_score)\n",
    "print (final_qa_score)\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
